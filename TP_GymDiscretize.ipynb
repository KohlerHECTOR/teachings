{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Gym ##\n",
    "gym is a python library that regroups many MDPs (https://www.gymlibrary.dev/) . It provides an abstract MDP class with a step() function for an RL agent to observe a tuple next_state, reward, is_terminal, some_infos ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to instantiate a MDP with gym: the inverted pendulum example\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "# env = gym.make(\"Acrobot-v1\")\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "#Initialize the MDP in a state.\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    #Take a random action in the MDP and observe next_s, r , done, _. \n",
    "    next_state, reward, done, info = env.step(env.action_space.sample())\n",
    "    #Rendering of the environment\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        next_state = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "It is important to note that the Pendulum is a continuous MDP: its State space is a hypervolume in |R^3 and its Action space is in |R .  \n",
    "Read the gym documentation (https://www.gymlibrary.dev/api/spaces/) and print the State and Action spaces of the inverted Pendulum MDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env. ...\n",
    "for i in range (state_space.shape[0]):\n",
    "    print(\"The \"+str(i)+\"th feature of the State space has values between \"+str(...) + \" and \"+ str(...))\n",
    "    \n",
    "action_space = env. ...\n",
    "print(\"The agent can actuate the pendulum engin with torque between \" +str(...) + \" and \" + str(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gym wrappers\n",
    "It is possible to use wrappers for gym environments in order to change what the agent will observe or, for example, to get from a continuous Action space to a discrete one.  \n",
    "For example, if the agent can take a continuous action between 0 and 1, then one can write an action wrapper so the agent can now take 2 discrete actions: the first discrete action is 0 and the other discrete action is 1.  \n",
    "Read the documentation https://www.gymlibrary.dev/api/wrappers/ and complete the code for the DiscreteActionWrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteActionWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, number_of_discr_actions = 2):\n",
    "        super().__init__(env)\n",
    "        self.n = number_of_discr_actions\n",
    "        \n",
    "        self.discrete_action_to_continuous = ...\n",
    "\n",
    "        self.discrete_action_space = gym.spaces.Discrete(len(self.discrete_action_to_continuous))\n",
    "\n",
    "    def step(self, action):\n",
    "        # the agent inputs a discrete action, we need to get the corresponding continuous action\n",
    "        a = self.discrete_action_to_continuous[action]\n",
    "        # we use the continuous action in the original continuous state MDP to get an observation \n",
    "        return self.env.step(a)\n",
    "    \n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "print(env.action_space)\n",
    "env = DiscreteActionWrapper(env, 2)\n",
    "print(env.action_space)\n",
    "print(\"The first discrete action corresponds to a torque of: \" + str(env.discrete_action_to_continuous[0]))\n",
    "print(\"The second discrete action corresponds to a torque of: \" + str(env.discrete_action_to_continuous[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use wrappers to change the State space of a MDP\n",
    "For example the State space of the Pendulum is Cos of the angle x Sin of the angle x Angular velocity in |R^3. But we can change it to be Angle x Angular velocity in |R^2 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AngleWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        #The angle is in radian between -pi and pi, and the angular velocity is between -8 and 8.\n",
    "        self.observation_space = gym.spaces.Box(np.array([-np.pi, -8]), np.array([np.pi, 8]))\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # The pendulum env has an attribute .state that is [current angle, current angular velocity]\n",
    "        return self.state\n",
    "    \n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "print(\"Continuous State space with 3 continuous features: \", env.observation_space)\n",
    "print(env.reset())\n",
    "env = AngleWrapper(env)\n",
    "print(\"Continuous State space with 2 continuous features: \", env.observation_space)\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Grid wrapper for the Pendulum\n",
    "Implement the GridWrapper for the Pendulum environment to get from a continuous State space to a discrete State space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, grid_size = 40):\n",
    "        super().__init__(env)\n",
    "        assert self.observation_space.shape[0] == 2, \"Environment does not have 2 continuous features.\"\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        ...\n",
    "\n",
    "        self.observation_space = gym.spaces.Discrete(self.grid_size * self.grid_size)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        ...\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework  \n",
    "1 ) Try Q-learning and Sarsa on the discrete states and discrete actions Pendulum (try different epsilons, different taus, different grid sizes ...).   \n",
    "2 ) Plot the Q-fonctions of some policies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
